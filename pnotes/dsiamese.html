<!DOCTYPE html>
<html>
  <head>
    <title>Learning Divergences</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      /* Slideshow styles */
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Siamese and Divergences

---

## Siamese Loss Function

$$E_W = \| G_W(X_1) - G_W(X_2) \|$$

Contrastive Loss Function:

$$ L(W,(Y, X_1, X_2)^i) = $$
$$ (1 - Y)L_G(E_W(X_1, X_2)^i) + YL_I (E_W (X_1, X_2^{'})^i) $$

---

class: center, middle

# Siamese Parallels

---

## Margin

### In siamese

![margin](http://i.imgur.com/0S2gseN.png)

### In d-siamese
Margin is the divergence

---


## Loss Functions

### In siamese
![loss function](http://i.imgur.com/v3U77CX.png)

### In d-siamese

In case of divergences:

- Requirement of a convex divergence measure

The loss function operates over the divergences for the three data points.
Optimization happens over functions (variational maximization).

---

## Future directions

  - Calculus of Variations
  - Survey of methods for density estimation


    </textarea>
    <script src="./remark-latest.min.js" type="text/javascript"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>

