<!DOCTYPE html>
<html lang="en-us">
  <head>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="A simple, minimal blog for those who love text.">
    <title>implement-progressive-gan.md | sahilmn</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="https://sahilmn.com/css/theme-override.css">
    <header>

  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="https://sahilmn.com">~/sahilmn</a>
      </li>
      

      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>



<div class="article-meta">
<h1><span class="title">implement-progressive-gan.md</span></h1>


<p class="terms">
  
  
  
  
  
</p>
</div>


<div class="content-wrapper">
  <main>
    <h1 id="progressive-gan-implementation">Progressive GAN Implementation</h1>
<p>Link (code): <a href="https://github.com/altairmn/progressive-gan">https://github.com/altairmn/progressive-gan</a></p>
<p>Paper: <a href="https://arxiv.org/abs/1710.10196">https://arxiv.org/abs/1710.10196</a></p>
<h3 id="overview">Overview</h3>
<p>Progressive GANs work by gradually adding layers to the discriminator and generator of the GAN <em>during training</em>. The benefits of this approach include faster training, reduced amortized memory and GPU usage over the course of training, and qualitatively better results. This paper is focused on training a GAN for generating from the distribution of CelebA-HQ dataset.</p>
<h3 id="notable-points">Notable Points</h3>
<ul>
<li>
<p><strong>Loss function</strong> is the wasserstein loss. Generator outputs an image from the toRGB layer, and the discriminator takes in an image in the fromRGB layer. toRGB layers: $n \to 3$ channels, fromRGB layers: $3 \to n$ channels.</p>
</li>
<li>
<p><strong>Generator Input</strong> input samples are from a 512 dimensional hyper sphere. So to sample for the generator, take a 512-dim random sample, and then normalize it to norm = 1</p>
</li>
<li>
<p>**Initialization: **  All weights in the network are initialized using a (0,1) normal distribution and bias with 0</p>
</li>
<li>
<p><strong>LCN (Local Response Norm)</strong> is used in the <!-- raw HTML omitted -->generator<!-- raw HTML omitted --> after every Conv layer. LCN normalizes each pixel slice across channels, and normalization constant is square root of average sum of squares. <a href="###use-of-local-response-norm">More on LCN</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">nn<span style="color:#f92672">.</span>LocalResponseNorm(size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress], alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>)
</code></pre></div></li>
<li>
<p><strong>Minibatch standard deviation layer</strong>: after the last layer of the discriminator, there is a minibatch standard deviation layer. it computes the standard deviation for each feature in each spatial location across a minibatch. Then all these values are averaged to arrive at a single value. Then this value is replicated to create an extra channel (making channels go from $512 \to 513$) and concatenated to the input to the discriminator output layer.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stddev</span>(self, x):
    <span style="color:#e6db74">&#34;&#34;&#34; Compute stddev
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
    y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(x<span style="color:#f92672">.</span>square()<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> x<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>square())
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>mean(y)
</code></pre></div></li>
<li>
<p><strong>Equalized Learning Rate</strong>: This is the most tricky part of the paper. The weights in the generator are scaled at runtime i.e. after each learning step, you&rsquo;d rescale the weights in the network. Each weight is set as $w_i = w_i/c$ where $c$ is the constant from He&rsquo;s initializer. In this implementation, we use <code>apply_elr</code> method to scale the weights after each cycle of training.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply_elr</span>(self):
    <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters():
        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;weight&#34;</span> <span style="color:#f92672">in</span> param[<span style="color:#ae81ff">0</span>]:
            param[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>data <span style="color:#f92672">=</span> param[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>data <span style="color:#f92672">*</span> getHeMultiplier(param[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>data)
</code></pre></div></li>
<li>
<p><strong>Transition</strong>: When adding a new layer to the discriminator and generator, there&rsquo;s an interim transition period to ensure that we don&rsquo;t start out cold with the new layers' weights. Transition for discriminator and generator is described <a href="###transition">here</a>.</p>
</li>
</ul>
<h3 id="architecture">Architecture</h3>
<p><!-- raw HTML omitted --></p>
<p>The boxed sizes are the outputs of the fromRGB layers.</p>
<p><img src="/home/altairmn/Projects/sahilmn.com/static/images/image-20210513231448917.png" alt="image-20210513231448917"></p>
<p>The boxed represent the input sizes to the toRGB layer</p>
<h3 id="transition">Transition</h3>
<p><img src="/home/altairmn/Projects/sahilmn.com/static/images/image-20210126215646007.png" alt="image-20210126215646007"></p>
<p>Each transition is two step:</p>
<ul>
<li><em>(b)</em> Add new layers, and setup a new input/output layer depending on discriminator or generator</li>
<li><em>(c)</em> Discard the old input/output layers i.e. fromRGB and toRGB layers</li>
</ul>
<p>To achieve this, this implementation uses the <code>alpha</code> value as an indicator of the step (b) or (c).</p>
<h4 id="transition-in-generator">Transition in Generator</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transition</span>(self):
    <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">==</span> <span style="color:#ae81ff">0.0</span>:
        self<span style="color:#f92672">.</span>progress <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>progress <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
        self<span style="color:#f92672">.</span>inputs<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>, nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,out_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
        self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>, nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],out_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],out_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>],kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
            nn<span style="color:#f92672">.</span>AvgPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)))
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Adding transition layers&#34;</span>)
        <span style="color:#66d9ef">else</span>:
            self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Completing transition&#34;</span>)
            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>progress
</code></pre></div><h4 id="transition-in-discriminator">Transition in Discriminator</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transition</span>(self):
    <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        self<span style="color:#f92672">.</span>progress <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>progress <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
        self<span style="color:#f92672">.</span>outputs<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],<span style="color:#ae81ff">3</span>,kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
        self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Upsample(scale_factor <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, mode <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;nearest&#39;</span>),
            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>],out_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
            nn<span style="color:#f92672">.</span>LocalResponseNorm(size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress], alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(negative_slope <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>),
            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],out_channels<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress],kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
            nn<span style="color:#f92672">.</span>LocalResponseNorm(size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>channel_dims[self<span style="color:#f92672">.</span>progress], alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(negative_slope <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>)
        ))
        <span style="color:#66d9ef">else</span>:
            self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</code></pre></div><p>Note how <code>alpha</code> value is toggled. The <code>progress</code> instance variable is useful to get the appropriate values of the dimensions for that stage of training.</p>
<p>‚ùó This implementation also uses an identity layer as a hidden layer to start for clean code. You can see it marked in the Discriminator architecture diagram.</p>
<h3 id="use-of-local-response-norm">Use of Local Response Norm</h3>
<p>To control the magnitudes in the generator, LRN is employed. It normalizes each pixel $a_{x,y}$ in the activation that the sum of $N$ neighboring pixels across channels is 1. This is done like so:
$$
b_{x,y} = a_{x,y}/\sqrt{\sum_{j=0}^{N-1} (a_{x,y}^j)^2 + \epsilon} \quad {\epsilon&gt;0, &laquo;&lt;1}
$$</p>
<p>Pytorch computes it in this way:
$$
b_{c} = a_{c}\left(k + \frac{\alpha}{n}
\sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}
$$
In this network, the whole vector across channels is normalized, so the layer is specified as</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">nn<span style="color:#f92672">.</span>LocalResponseNorm(size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>n, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>) <span style="color:#75715e"># n is number of channels</span>
</code></pre></div><blockquote>
<p>we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer.</p>
</blockquote>
<p>In this implementation, we apply the normalization <em>after</em> the leaky ReLU layer, and not before.</p>

    <a href="/"> >> Home</a>
  </main>
</div>
    <footer>
      
<script>
(function() {
  function center_el(tagName) {
    var tags = document.getElementsByTagName(tagName), i, tag;
    for (i = 0; i < tags.length; i++) {
      tag = tags[i];
      var parent = tag.parentElement;
      
      if (parent.childNodes.length === 1) {
        
        if (parent.nodeName === 'A') {
          parent = parent.parentElement;
          if (parent.childNodes.length != 1) continue;
        }
        if (parent.nodeName === 'P') parent.style.textAlign = 'center';
      }
    }
  }
  var tagNames = ['img', 'embed', 'object'];
  for (var i = 0; i < tagNames.length; i++) {
    center_el(tagNames[i]);
  }
})();
</script>

      
      <hr/>
      Open-Source | <a href="https://github.com/goodroot/hugo-classic">Github</a> | <a href="https://keybase.io/goodroot">Keybase</a>
      
    </footer>
    <script>
(function(f, a, t, h, o, m){
	a[h]=a[h]||function(){
		(a[h].q=a[h].q||[]).push(arguments)
	};
	o=f.createElement('script'),
	m=f.getElementsByTagName('script')[0];
	o.async=1; o.src=t; o.id='fathom-script';
	m.parentNode.insertBefore(o,m)
})(document, window, '//analytics.example.com/tracker.js', 'fathom');
fathom('set', 'siteId', 'ABCDE');
fathom('trackPageview');
</script>

  </body>
</html>

